{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e48d49-de77-48d5-8ac2-0ef7191ecf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scenario:\n",
    "\n",
    "You are working on a data integration project where you need to consolidate user, order, and product information from multiple JSON files into a single merged JSON file. Your task involves several steps, including schema inference, data transformation, and writing results to a data catalog table. Identify the difference in schema and final dataframe and generate the output. \n",
    "\n",
    "Requirements:\n",
    "\n",
    "Schema Inference:\n",
    "\n",
    "You have a CSV file named schema.csv that describes the schema of the final DataFrame. The CSV file is stored in an S3 bucket at s3://your-bucket/schema.csv. Using AWS Glue Crawler, infer the schema from the schema.csv file and create a table in the Glue Data Catalog.\n",
    "\n",
    "Data Transformation:\n",
    "\n",
    "You have three JSON files stored in S3: users.json (located at s3://your-bucket/users.json) orders.json (located at s3://your-bucket/orders.json) products.json (located at s3://your-bucket/products.json) These JSON files contain user, order, and product information respectively.\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "Flatten the JSON structures. Join the data based on appropriate keys. Produce a single merged JSON file.\n",
    "\n",
    "Write to Glue Table:\n",
    "\n",
    "Write the resulting merged JSON data to a table in the Glue Data Catalog. The table should be named merged_data_table and should be stored in the S3 location s3://your-bucket/merged_data/.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da037ab-f800-4ea2-9289-bb742aebe8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c1c66-9e9b-4fd6-b6d6-57a7847d3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark and Glue contexts\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = SparkSession.builder.appName(\"DataIntegration\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485e092-5cf6-400a-828b-4da2cd131dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema CSV from S3\n",
    "s3_client = boto3.client('s3')\n",
    "schema_bucket = 'project_input_data_integration_bucket'\n",
    "schema_key = 'schema.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e48f0-039b-4f7a-9a02-309842e64fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download schema file\n",
    "schema_obj = s3_client.get_object(Bucket=schema_bucket, Key=schema_key)\n",
    "schema_df = pd.read_csv(schema_obj['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5401095-ce5a-414e-b7c5-0eb68f17dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer schema for the final DataFrame\n",
    "schema_inferred = schema_df.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23694a6-087b-4d14-b5d2-3caf742c65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON files from S3\n",
    "users_json_path = 's3://your-bucket/users.json'\n",
    "orders_json_path = 's3://your-bucket/orders.json'\n",
    "products_json_path = 's3://your-bucket/products.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2d10b-8563-4d27-94d7-364264f15460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load users, orders, and products JSON data\n",
    "users_df = spark.read.json(users_json_path)\n",
    "orders_df = spark.read.json(orders_json_path)\n",
    "products_df = spark.read.json(products_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff8a49-1ef9-45fb-85ee-a2da30438e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested JSON structures (for users and orders)\n",
    "users_flat_df = users_df.withColumn(\"home_street\", col(\"address.home.street\")) \\\n",
    "    .withColumn(\"home_city\", col(\"address.home.city\")) \\\n",
    "    .withColumn(\"home_zipcode\", col(\"address.home.zipcode\")) \\\n",
    "    .withColumn(\"office_street\", col(\"address.office.street\")) \\\n",
    "    .withColumn(\"office_city\", col(\"address.office.city\")) \\\n",
    "    .withColumn(\"office_zipcode\", col(\"address.office.zipcode\")) \\\n",
    "    .drop(\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecf816-47bc-418f-8808-0622b70568f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_flat_df = orders_df.withColumn(\"item\", explode(col(\"items\"))) \\\n",
    "    .withColumn(\"product_name\", col(\"item.product_name\")) \\\n",
    "    .withColumn(\"quantity\", col(\"item.quantity\")) \\\n",
    "    .withColumn(\"price\", col(\"item.price\")) \\\n",
    "    .drop(\"items\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a90da-7cd5-434b-a080-477f62415fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data on appropriate keys (user_id, product_name)\n",
    "merged_df = orders_flat_df.join(users_flat_df, orders_flat_df.customer_id == users_flat_df.user_id, \"inner\") \\\n",
    "    .join(products_df, orders_flat_df.product_name == products_df.product_name, \"inner\") \\\n",
    "    .select(\"order_id\", \"customer_id\", \"first_name\", \"last_name\", \"product_name\", \"quantity\", \"price\", \"total_amount\",\n",
    "            \"home_street\", \"home_city\", \"home_zipcode\", \"office_street\", \"office_city\", \"office_zipcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5cd9d-38f4-4190-a1ac-8345421303bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final merged DataFrame\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f2aee-a365-454c-9fdd-e25161185101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the resulting merged JSON data to a Glue Table\n",
    "output_path = 's3://your-bucket/merged_data/'\n",
    "merged_df.write.json(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f066f56-d80e-44c4-9eb3-32af9e88d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Glue Table and save the merged data\n",
    "glueContext.create_dynamic_frame.from_catalog(database=\"final_database\", table_name=\"merged_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b205f-854d-44e5-ac3e-baa996a57000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900b69f-3195-4bd1-afe8-f9b17c84fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
